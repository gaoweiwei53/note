# 1. Hadoop
## 1. hadoop常用端口号？
hadoop3.x 
- HDFS NameNode 内部通常端口：8020/9000/9820
- HDFS NameNode 对用户的查询端口：9870
- Yarn查看任务运行情况的：8088
- 历史服务器：19888

## 2. HDFS文件块大小？
128M
## 3. HDFS的读写流程（面试重点）？

## 4. MapReduce的工作流程

## 5. Yarn的工作机制（面试题）

## 6. HDFS安全模式
安全模式是hadoop的一种保护机制，用于保证集群中的数据块的安全性。当集群启动的时候，会首先进入安全模式。当系统处于安全模式时会检查数据块的完整性。检查副本率小于设置的则会复制block到其他datanode中。
## 7. 什么是命名空间 namespace
hdfs文件系统里的文件夹、文件、文件结构(文件系统树)一起构成了namespace，metadata指的是这些文件夹或文件的属性。
# 2. Kafka
## 1. Partition 和 Replica
Producer产生的消息发布到topic里，Event会按照相同的**event key**追加到这个Topic的其中同一个Partition上。每一个Partition都会有若个Replica，这些Replica分布在不同的broker上，且每个broker至多只能有一个同一个Partition对应的Replica。所以一个topic的replica的数量不会多于broker的数量。

一个Partition的所有的Replica中，有一个是leader，其他都是follower。读写消息的过程只通过leader完成。

客户端可以控制向哪个partiton发送消息。默认的是随机发送，这样可以负载平衡。用户也可以自定义分区函数，例如使相同的user id的消息hash到同一个partition.
## 2. Pull or Push
Producer *Push* 到broker，然后由consumer 从broker里 *pull* 消息 
### Push方式
优点：Consumer可以以最大速率消费数据。  
缺点：broker不知道下游的消费者是否能够立即消费，它要决定是立即发送消息还是积累数据等待一段时间再发送。
### Pull方式
优点：consumer可以自定义消费速率; 能更好地以批方式拉取数据而不会导致延迟  
缺点：当broker没有消息时，consumer会等待。解决办法：设置等待的时间

## 3. kafka如何保证一致性语义？
### 3.1 消息传送保证
- At most once：最多发送一次，丢失不重发，会丢失数据。
- At least once：只发送一次，丢失会重发，所以永远不会丢失数据。
- Exactly once：每个数据只会且会被发送一次。
> 对于kafka包括*发布消息时地一致性保证*和*消费消息时的一致性保证*
## 3.2 发布消息时地一致性保证
当发布消息时，有一个“Committed”到日志里的概念。当消息是Committed时，只要它所在的broker存活就不会丢失。

在0.11.0.0版本之前，producer发布消息网络发生故障时，如果producer没有收到消息被committed的回复，那么它就会重发。这样可能会导致数据重复写入。

从0.11.0.0版本开始，kafka提供了**幂等( idempotent)发送**操作，可以保证重发的消息不会被重复写入。为了实现这一点，broker为每个生产者分配一个ID，并使用生产者连同每个消息一起发送的序列号对消息进行重复数据删除。

并且从0.11.0.0版本开始，生产者支持使用类似事务的语义将消息发送到多个主题分区的能力，即要么所有消息都被成功写入，要么没有消息被成功写入。

并不是所有的情况都需要这样强有力的保证。对于延迟敏感的场景，Producer可指定它想要的持久性级别。如果生产者指定它希望等待提交的消息，例如可指定10毫秒。然而，生产者也可以指定它希望完全异步地执行发送，或者它希望只等待leader(但不一定是follower)有确认消息。
## 3.3 消费消息时的一致性
所有replica都具有具有相同偏移量的完全相同的日志。如果消费者从未崩溃，它可以将这个offset存储在内存中，但如果消费者崩溃，topic分区被另一个进程接管，新进程将需要选择一个合适的offset开始消费。它有几个处理消息和更新其offset的选择。

当写入到外部系统时，需要协调consumer的位置和输出。经典的做法是**两段提交**，然而并不是所有的外部系统都支持两端提交。另一种简单的做法是让consumer把它的offset存储在与输出地方的相同位置。例如将数据和读取到的offset一起导入到HDFS中，这样就可以保证数据和偏移量都被更新或者都不更新。

## 3.4 副本 
 leader维护着"in sync"列表，当有follower崩了或者消息落后leader很多时，leader就会将这个follower从ISA中移除。现在可以更精确地定义，当该分区的所有同步副本都将消息写到到日志中时，该消息被认为已提交。只有已提交的消息才会分发给consumer。另一方面，生产者可以选择等待消息提交或不提交，这取决于他们在延迟和持久性之间的权衡。该选择由producer使用的acks设置控制。注意，topic有一个`minmum number` 的同步副本设置，当producer请求确认: 消息已经被写入ISA中所有的同步副本时，会检查该设置。如果生产者请求的是一个不那么严格的确认，即使同步中的副本数量低于最小值(例如，它可以低到只有leader)，那么消息可以被提交和消费，
## 3.5 Replicated Log
日志复制算法必须提供的基本保证是，如果告诉客户端一条消息已经提交，但同时leader不可用了，我们选举的新leader必须也有这个消息。当有多个follower时，就要权衡选举哪个follower作为leader。对于这个权衡，一个常用的方法是对**提交决定**和**leader选举**都采用**多票法**。这个方法族包括很多算法，如ZooKeeper的**Zab**, **Raft**和 **Viewstamped Replication**. 我们所知道的与Kafka的实际实现最相似的是微软的**PacificA**.

多数投票的缺点是，不需要太多的失败就会让你失去可选举的领导人。容忍一次故障需要数据的三份副本，容忍两次故障需要数据的五份副本。根据我们的经验，只有足够的冗余来容忍一次故障对于实际系统来说是不够的，但是使用5倍的磁盘空间需求和1/5的吞吐量，每写5次，对于大容量数据问题来说是不太实际的。这可能就是为什么仲裁算法(Quorum)在共享集群配置(如ZooKeeper)中更常见，但在主数据存储中不太常见的原因。例如，在HDFS中，namenode的高可用性特性是建立在基于多数投票的日志上的，但是这种更昂贵的方法并不用于数据本身。

Kafka在选择仲裁集时采取了一种稍微不同的方法。Kafka不是采用多数投票的方式，而是动态地维护一组与leader同步的副本(ISR)。只有这一组成员才有资格当选为领导人。对Kafka分区的写操作只有在所有同步副本都收到写操作后才被认为已提交。当ISR集发生变化时，它就会被保存到ZooKeeper中。正因为如此，ISR中的任何副本都有资格当选领导人。有了这个ISR模型和f+1副本，Kafka主题可以容忍f失败而不会丢失提交的消息。
## 如果都失败了怎么办？
有两种选择：
- 等待ISA中的一个副本复活，选择这个副本作为leader
- 选择最先复活的副本 (不需要在ISA中)作为leader.

两个选择各有优缺点：
- 选择1的话，如果ISA中的副本如果一致不复活，就要一直等下去。如果这些副本被破坏或数据丢失，那么将永久停机。
- 选择2的话，意味着要选择该副本里的数据作为参照标准，但是这个副本的数据完整性不能保证。

## ACK
Producer可以通过设置`acks = 0,1, -1(all)`, 选择是否等待, 消息被replicas确认。
- `acks = -1(all)`:当前ISA中的所有副本接收到消息时立即确认。
- `acks = 1`:
- `acks = 0`:

kafka提供了两种topic-level配置，可用于偏好消息持久性而非可用性:
- 禁用不干净的leader选举-如果所有副本都不可用，那么这个分区将一直不可用，直到最近的leader重新可用。
- 指定一个最小的ISR大小——只有当ISR的大小超过某个最小值时，该分区才会接受写操作，以防止仅写入单个副本的消息丢失，而该副本随后将不可用。只有当生产者使用acks=all并保证消息至少被这么多同步副本确认时，该设置才会生效。这种设置提供了一致性和可用性之间的权衡。设置较高的最小ISR大小可以保证更好的一致性，因为可以保证消息被写入更多的副本，从而减少消息丢失的概率。但是，它会降低可用性，因为如果同步副本的数量低于最小阈值，该分区将无法进行写操作。
## 副本管理
上面关于复制日志的讨论实际上只涵盖了单个日志，即一个主题分区。然而，Kafka集群将管理成百上千个这样的分区。我们试图以循环方式平衡集群中的分区，以避免将大容量主题的所有分区群集在少量节点上。同样地，我们试图平衡领导位置，以便每个节点都是其分区的比例份额的成为领导

优化领导选举过程也很重要。一个简单的leader选举实现是在节点失败时为每个分区执行一个leader选举。

不同的是kafka选择其中一个broker作为“Contoller”。该controller在broker-level检测故障，并负责更改故障broker中所有受影响分区的leader。结果是能够批量处理许多需要的leader变更通知，这使得选举过程对大量的分区来说更加方便和快速。

如果controller发生故障，则一个幸存的broker将成为新的controller。
# Java
## 1. Java基本数据类型几个？
Java提供了八种基本类型。六种数字类型（四个整数型，两个浮点型），一种字符类型，还有一种布尔型。
- byte   1字节
- short  2字节
- int    4字节
- long   8字节
- float  4字节
- double 8字节
- char   2字节
- boolen 表示1位的信息
## 2. StringBuffer和StringBuilder的区别？
和String类不同的是，StringBuffer 和 StringBuilder 类的对象能够被多次的修改，并且不产生新的未使用对象。
- StringBuffer 和 StringBuilder 长度可变

- StringBuffer 线程安全 StringBuilder 线程不安全

- StringBuilder 速度快，推荐使用
